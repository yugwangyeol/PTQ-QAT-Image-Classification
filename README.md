# PTQ-QAT-Image-Classification

<div align="center">
  <img src="https://github.com/user-attachments/assets/553f1cc5-603a-4f45-a3f3-3a025d647df9" alt="PTQ-QAT Comparison">
</div>

본 실험에서는 **CIFAR10 데이터셋**에 대해 Base 모델, PTQ(Post-Training Quantization) 모델, 그리고 QAT(Quantization-Aware Training) 모델의 성능을 **정확도**, **모델 크기**, **추론 시간** 측면에서 비교 분석하였다.

---

### [Result - Accuracy]
- **Base 모델**: 76.63%의 정확도를 달성하며, 양자화되지 않은 원본 모델의 성능을 나타냄.
- **PTQ 모델**: 69.34%로 정확도가 크게 감소. 이는 양자화 과정에서 발생하는 정보 손실의 영향이 큼을 보여줌.
- **QAT 모델**: 74.93%의 정확도를 기록하며, Base 모델 대비 약 **1.7%p 감소**에 그침. 이는 PTQ 대비 우수한 성능 보존 능력을 입증.

### [Result - Parameter Size]
- Base 모델 크기: **5.96MB**
- PTQ/QAT 모델 크기: **1.51MB** (Base 모델 대비 **74.6% 감소**)
- 두 양자화 모델 모두 동일한 크기 감소율을 보이며, 엣지 디바이스 배포 시 저장 공간 효율성을 제공.

### [Result - Inference time]
- Base 모델 추론 시간: **385.82 ± 72.12ms**
- PTQ 모델 추론 시간: **182.55 ± 39.81ms** (**52.7% 속도 향상**)
- QAT 모델 추론 시간: **185.02 ± 44.61ms** (**52.1% 속도 향상**)
- 두 양자화 모델은 Base 모델 대비 약 **2배 빠른 추론 속도**를 보여, 실시간 처리가 중요한 환경에서 큰 이점을 제공.

---

### [Conclusion]
실험 결과를 종합해보면, **QAT**는 전반적으로 가장 균형 잡힌 성능을 보여주었다.
- QAT는 Base 모델 대비 **근소한 정확도 감소**로도 **모델 크기를 74.6% 줄이고**, **추론 속도를 52.1% 개선**시킴.
- PTQ는 크기 감소 및 속도 향상 측면에서는 QAT와 유사했으나, **정확도 손실이 더 큼**.

따라서:
- **정확도가 중요한 애플리케이션**: QAT가 적합.
- **빠른 구현과 정확도 손실을 감수**할 수 있는 경우: PTQ가 적합.

두 방법 모두 모델 경량화와 추론 속도 개선이라는 목표를 효과적으로 달성하였으며, 활용 시나리오에 따라 적절한 선택이 가능할 것으로 보인다.
